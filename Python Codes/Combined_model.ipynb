{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Combined code Major project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frCocvqcB9CZ"
      },
      "source": [
        "#BERT Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn1fyrR5A4sR"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHskqnT6DZSZ"
      },
      "source": [
        "#Bert tokenization class\n",
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp2rukpQQOSd"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import tokenization\n",
        "import tensorflow_hub as hub\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBEFVKRhPDU"
      },
      "source": [
        "#model with adam optimizer\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "sgd = keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "adadelta = keras.optimizers.Adadelta(learning_rate=1.0, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0l8__KvQmAr"
      },
      "source": [
        "#Builiding BERT layer\n",
        "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNCpcBg9QwvT"
      },
      "source": [
        "#Reading train.jsonl\n",
        "train_df = pd.read_json('/content/drive/MyDrive/data/train.jsonl', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9u11Dvxj8A6"
      },
      "source": [
        "#Readinf dev_seen.jsonl\n",
        "dev_seen_df = pd.read_json('/content/drive/MyDrive/data/dev_seen.jsonl', lines=True)\n",
        "\n",
        "#Concatenating train_df and dev_seen_df\n",
        "training_data = pd.concat([train_df, dev_seen_df])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRkvn-80Cu7A"
      },
      "source": [
        "#Validation Data\n",
        "dev_df = pd.read_json('/content/drive/MyDrive/data/dev_unseen.jsonl', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhReVgPSBb4s"
      },
      "source": [
        "#Splitting the data into training and testing\n",
        "df_train, df_test = train_test_split(\n",
        "    training_data,\n",
        "    test_size=0.05,\n",
        "    random_state=0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMXO3W8TWBP"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63azlgFPT7q4"
      },
      "source": [
        "#Encoding the text(preprocessing)\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdQXGWlKVEkY"
      },
      "source": [
        "#defining the model\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    net = tf.keras.layers.Dropout(0.2)(clf_output)\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
        "    out = tf.keras.layers.Dense(2, activation='sigmoid')(net)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePta_UQEW_l7"
      },
      "source": [
        "train_input = bert_encode(df_train.text.values, tokenizer, max_len=100)\n",
        "test_input = bert_encode(df_test.text.values, tokenizer, max_len=100)\n",
        "traiin_labels = tf.keras.utils.to_categorical(df_train.label.values, num_classes=2)\n",
        "test_labels =  tf.keras.utils.to_categorical(df_test.label.values, num_classes=2)\n",
        "\n",
        "\n",
        "dev_input = bert_encode(dev_df.text.values, tokenizer, max_len=100)\n",
        "dev_labels = tf.keras.utils.to_categorical(dev_df.label.values, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBKDRQbkX3VQ"
      },
      "source": [
        "text_model = build_model(bert_layer, max_len=100)\n",
        "text_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrD2rYUEYkf3"
      },
      "source": [
        "#run model\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = text_model.fit(\n",
        "    train_input, traiin_labels, \n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebkwV9IqZMbZ"
      },
      "source": [
        "#Predict\n",
        "text_model.load_weights('model.h5')\n",
        "test_pred = text_model.predict_generator(test_input, steps=450)\n",
        "\n",
        "test_pred = np.argmax(test_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ_ymjJorHXF"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kFO06kLkRce"
      },
      "source": [
        "#training validation accuracy graph\n",
        "plt.plot(train_history.history['accuracy'], label='training acc')\n",
        "plt.plot(train_history.history['val_accuracy'], label='validation acc')\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPz2IPKZlovA"
      },
      "source": [
        "print(classification_report(df_test.label, test_pred, target_names=['Non-Offensive(0)','Offensive(1)']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPknwjL7kBMQ"
      },
      "source": [
        "#Confusion Matrix\n",
        "\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "cm = confusion_matrix(df_test.label, test_pred)\n",
        "df_cm = pd.DataFrame(cm, index=['Non-Offensive(0)','Offensive(1)'], columns=['Non-Offensive(0)','Offensive(1)'])\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg6_zQPBTEPm"
      },
      "source": [
        "#VGG16 image model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3-OeNnx7eaA"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxKG49KY7eaB"
      },
      "source": [
        "#image dimensions\n",
        "img_width, img_height = 224,224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52m-pVV27eaB"
      },
      "source": [
        "train_data_dir = \"/content/drive/MyDrive/data/train_data\"\n",
        "validation_data_dir = \"/content/drive/MyDrive/data/dev_seen_data\"\n",
        "# nb_train_samples = 2000\n",
        "# nb_validation_samples = 100\n",
        "# epochs = 50\n",
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfaMaUCe7eaC"
      },
      "source": [
        "#image data is represented in three dim-array where first channel represents the colour channels:[channels][rows][columns]\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "  input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w_5ka7z7eaD"
      },
      "source": [
        "#Wrapper for preprocess_input() to make it compatible to use with ImageDataGenerator's preprocessing_function\n",
        "def preprocess_vgg(x):\n",
        "  X = np.expand_dims(x, axis=0)\n",
        "  X = preprocess_input(X)\n",
        "  return X[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-eKcuM7eaD"
      },
      "source": [
        "#Intializing VGG16 with Imagenet weights\n",
        "vgg16 = VGG16(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyMRetIi7eaE"
      },
      "source": [
        "x = vgg16.get_layer('fc2').output\n",
        "prediction = Dense(2, activation='softmax', name='predictions')(x)\n",
        "\n",
        "img_model = Model(inputs=vgg16.input, outputs=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83W0eIrX7eaE"
      },
      "source": [
        "#freezing all the layers except bottlenecj layer for fine tuning\n",
        "for layer in img_model.layers:\n",
        "  if layer.name in ['predictions']:\n",
        "    continue\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI24XNdv7eaF"
      },
      "source": [
        "df = pd.DataFrame(([layer.name, layer.trainable] for layer in img_model.layers), columns=['layer','trainable'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGZwZ0nD7eaF"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_vgg,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(directory=train_data_dir,\n",
        "                                                    target_size=[img_width, img_height],\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdkk7IND7eaG"
      },
      "source": [
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_vgg)\n",
        "validation_generator = validation_datagen.flow_from_directory(directory=validation_data_dir,\n",
        "                                                              target_size=[img_width, img_height],\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LO_AVJ87eaG"
      },
      "source": [
        "#compile SGD optimizer with small learning rate\n",
        "sgd = SGD(lr=1e-4, momentum=0.9)\n",
        "img_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bl_henG7eaH"
      },
      "source": [
        "plot_model(img_model, to_file='/content/VGG16_img_model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUUutoOs7eaH"
      },
      "source": [
        "history = img_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=2000 // batch_size,\n",
        "    epochs=2,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps=100 // batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCXzYOLG7eaI"
      },
      "source": [
        "img_model.save_weights('/content/drive/MyDrive/data/vgg16_hateful_nonhateful_dense2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT_zEPaa7eaI"
      },
      "source": [
        "model_json_final = img_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/data/vgg16_hateful_nonhateful_dense2.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smj8RQGx7eaJ"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0-OG_VS4MyE"
      },
      "source": [
        "test_data_dir = \"/content/drive/MyDrive/data/dev_unseen_data\"\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_vgg)\n",
        "test_generator = test_datagen.flow_from_directory(directory=test_data_dir,\n",
        "                                                              target_size=[img_width, img_height],\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7tHeRy86BPa"
      },
      "source": [
        "#Confusion Matrix and Classification Report\n",
        "Y_pred = img_model.predict_generator(test_generator, steps=540)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = ['Non-Offensive', 'Offensive']\n",
        "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_UC37pRCulU"
      },
      "source": [
        "#Combined model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPI9tv8huOrz"
      },
      "source": [
        "\n",
        "#Importing all the libraries needed\n",
        "import keras\n",
        "import h5py\n",
        "from keras import optimizers, preprocessing, Input\n",
        "from keras.models import load_model, Model\n",
        "from keras.layers import Bidirectional\n",
        "#from multimodel baseline functions\n",
        "from keras.layers.core import Reshape, Dropout\n",
        "from keras.utils.vis_utils import  plot_model\n",
        "import os\n",
        "import itertools\n",
        "#import keras matrics\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling3D\n",
        "from keras import regularizers\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk import word_tokenize\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM, multiply\n",
        "from PIL import Image, ImageFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75Hfx5UpbZR"
      },
      "source": [
        "#training_path, testing_path and validation_path are for the text extracted from the meme images respectively in a .csv file\n",
        "Training_path = '/content/drive/MyDrive/data/train.jsonl'\n",
        "Testing_path = '/content/drive/MyDrive/data/dev_unseen.jsonl'\n",
        "Validation_path = '/content/drive/MyDrive/data/dev_seen.jsonl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja3IiiSzsmdW"
      },
      "source": [
        "img_dir = '/content/img' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnAHQiKyswuM"
      },
      "source": [
        "#directory for storing glove embeddings\n",
        "GLOVE_DIR = \"/content/drive/MyDrive/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqfNWlNus0wN"
      },
      "source": [
        "#assigning weight\n",
        "class_weight = {1: 1.4,\n",
        "                0: 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwmF9ilVs-Ux"
      },
      "source": [
        "def encode_label(DataFrame, Label_col):\n",
        "    t_y = DataFrame[Label_col].values\n",
        "    Encoder = LabelEncoder()\n",
        "    y = Encoder.fit_transform(t_y)\n",
        "    DataFrame[Label_col] = y\n",
        "\n",
        "def preprocess_text(Training_path,Validation_path, Testing_path):\n",
        "    # function to preprocess input\n",
        "    training_DF = pd.read_json(Training_path, lines = True)\n",
        "    validation_DF = pd.read_json(Validation_path, lines = True)\n",
        "    testing_DF = pd.read_json(Testing_path, lines = True)\n",
        "\n",
        "    # encoding all the labels \n",
        "    # encode_label(testing_DF,'label')\n",
        "    encode_label(training_DF, 'label')\n",
        "    encode_label(validation_DF, 'label')\n",
        "\n",
        "    return training_DF, testing_DF, validation_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWp-qKxWtQh9"
      },
      "source": [
        "#splitting data into train, test and validation\n",
        "training_df, testing_df, validation_df = preprocess_text(Training_path, Testing_path, Validation_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSJualp8u0WL"
      },
      "source": [
        "def create_img_array(img_dirct):\n",
        "    all_imgs = []\n",
        "    for root, j, files in os.walk(img_dirct):\n",
        "        for file in files:\n",
        "            file = root + '/' + file\n",
        "            all_imgs.append(file)\n",
        "    return all_imgs\n",
        "\n",
        "def create_img_path(DF, Col_name):\n",
        "    img_path = ['/content' + '/' + name for name in DF[Col_name]]\n",
        "    return img_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0Q8JuAZu6BL"
      },
      "source": [
        "\n",
        "# Processing image and text for each set\n",
        "# Creating train, test and validation image path\n",
        "train_img_path = create_img_path(training_df,'img')\n",
        "test_img_path = create_img_path(testing_df,'img')\n",
        "val_img_path = create_img_path(validation_df,'img')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krnp8M_7vLHt"
      },
      "source": [
        "# Vectorising text\n",
        "# process the whole observation into single list\n",
        "train_text_list=list(training_df['text'])\n",
        "test_text_list = list(testing_df['text'])\n",
        "val_text_list = list(validation_df['text'])\n",
        "\n",
        "# Creating vectors for train, test, validation\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(train_text_list)\n",
        "sequences_train = tokenizer.texts_to_sequences(train_text_list)\n",
        "sequences_test = tokenizer.texts_to_sequences(test_text_list)\n",
        "sequences_val = tokenizer.texts_to_sequences(val_text_list)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=100)\n",
        "x_test = preprocessing.sequence.pad_sequences(sequences_test, maxlen=100)\n",
        "x_val = preprocessing.sequence.pad_sequences(sequences_val, maxlen=100)\n",
        "\n",
        "# encoding all the labels \n",
        "y_test = testing_df['label']\n",
        "y_train = training_df['label']\n",
        "y_val = validation_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7HPBY5vscH"
      },
      "source": [
        "def get_input(path):\n",
        "    # Loading image from given path\n",
        "    # and resizing it to 224*224*3 format\n",
        "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "    img = image.load_img(path, target_size=(224,224))    \n",
        "    return(img)\n",
        "\n",
        "def process_input(img):\n",
        "    # Converting image to array    \n",
        "    img_data = image.img_to_array(img)\n",
        "    # Adding one more dimension to array    \n",
        "    img_data = np.expand_dims(img_data, axis=0)\n",
        "    #     \n",
        "    img_data = preprocess_input(img_data)\n",
        "    return(img_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EBvEZNapQKn"
      },
      "source": [
        "def img_text_generator(files, padded_seq, y, batch_size=None):\n",
        "    while True:\n",
        "        batch_idxs = np.random.choice(a = list(range(len(padded_seq))), size=batch_size) #Selecting the random batch indexes    \n",
        "        batch_input_txt = [] # Initializing batch input text\n",
        "        batch_input_img = [] # Initializing batch input image\n",
        "        batch_output = [] # Initializing batch output\n",
        "        \n",
        "        # Traversing through the batch indexes\n",
        "        for batch_idx in batch_idxs:\n",
        "            input_txt = padded_seq[batch_idx] # selecting padded sequences from the batch\n",
        "            output = y[batch_idx] # Selecting label  \n",
        "            input_img = get_input(files[batch_idx])\n",
        "            input_img = process_input(input_img)\n",
        "            batch_input_txt.append(input_txt) # Appending the input (text vector)\n",
        "            batch_input_img.append(input_img[0])\n",
        "            batch_output.append(output) # Appending the label\n",
        "        \n",
        "        # Return a tuple of (input,output) to feed the network\n",
        "        batch_x1 = np.array( batch_input_img )\n",
        "        batch_x2 = np.array( batch_input_txt )\n",
        "        batch_y = np.array( batch_output )\n",
        "        yield ([batch_x1, batch_x2], batch_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT3_FRZkwDfr"
      },
      "source": [
        "# Creating train, test, val, generator for meme\n",
        "img_txt_gen_train = img_text_generator(train_img_path, x_train, y_train, batch_size=32)\n",
        "img_txt_gen_val = img_text_generator(val_img_path, x_val, y_val, batch_size=1)\n",
        "img_txt_gen_test = img_text_generator(test_img_path, x_val, y_val, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug9cAdcHmBPg"
      },
      "source": [
        "text_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "img_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPmztzm1DG2S"
      },
      "source": [
        "class WeightedAverage(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, n_output):\n",
        "        super(WeightedAverage, self).__init__()\n",
        "        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n",
        "            trainable=True) # (1,1,n_inputs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
        "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
        "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
        "        inputs = keras.layers.Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
        "        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n",
        "        # weights sum up to one on last dim\n",
        "\n",
        "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbik9ov3Cwlo"
      },
      "source": [
        "import keras\n",
        "#concatenating the output of both the classifiers(text and image)\n",
        "con_layer = [text_model.output, img_model.output]\n",
        "W_Avg = WeightedAverage(n_output=len(con_layer))(con_layer)\n",
        "out = Dense(1, activation='sigmoid')(W_Avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTQRZ1-vmgf0"
      },
      "source": [
        "#Common Model\n",
        "from keras import optimizers\n",
        "\n",
        "com_model = Model(inputs = [img_model.input, text_model.input], outputs=out)\n",
        "\n",
        "com_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-kSdjRN8uPu"
      },
      "source": [
        "com_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOGitTrdDeW0"
      },
      "source": [
        "plot_model(com_model, to_file='Common_model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO9LAfKBnXjy"
      },
      "source": [
        "#Training the combined model\n",
        "combine_model = com_model.fit(img_txt_gen_train, epochs=3, validation_steps = 149, steps_per_epoch=2, validation_data=img_txt_gen_val, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyTmwa7wtA7"
      },
      "source": [
        "com_model.load_weights('/content/drive/MyDrive/data/Combined_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8iE70nIAM1p"
      },
      "source": [
        "y_true = y_test.values\n",
        "y_pred_com = (com_model.predict_generator(img_txt_gen_test,steps=540))\n",
        "y_pred_com = np.round(list(itertools.chain(*y_pred_com)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpLCVIqKB88f"
      },
      "source": [
        "labels = [1,0]\n",
        "cm = confusion_matrix(y_true, y_pred_com, labels)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['offensive', 'non-offensive']); ax.yaxis.set_ticklabels(['offensive', 'non-offensive']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOzbw48nCEZr"
      },
      "source": [
        "print(classification_report(y_true, y_pred_com, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGpgiqSDrmS7"
      },
      "source": [
        "plt.plot(combine_model.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(combine_model.history['val_accuracy'], label='Validation accuracy')\\\n",
        "\n",
        "plt.title('Training v/s Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfandJjvkJ-x"
      },
      "source": [
        "nb_sample = 4\n",
        "for x,y in zip(img_text_gen_test[:nb_sample], y_pred[:nb_sample]):\n",
        "  s = pd.Series({'Non-Offensive':1-np.max(y), 'Offensive':np.max(y)})\n",
        "  axes = s.plot(kind='bar')\n",
        "  axes.set_xlabel('Class')\n",
        "  axes.set_ylabel('Probability')\n",
        "  axes.set_ylim([0,1])\n",
        "  plt.show()\n",
        "\n",
        "  img = array_to_img(x)\n",
        "  display(img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}