{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec+BiLSTM",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4HkXfwna8B"
      },
      "source": [
        "#Word2Vec+BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Czt045HMJ5A"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz-nugX_MbAA"
      },
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEBMKzkkMo8y"
      },
      "source": [
        "#reading train.jsonl file\n",
        "df1 = pd.read_json('train.jsonl', lines = True)\n",
        "df1.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVefTX9At_sY"
      },
      "source": [
        "#reading dev_seen.jsonl file\n",
        "df2 = pd.read_json(\"dev_seen.jsonl\", lines=True)\n",
        "\n",
        "#concate the train adn dev_seen data\n",
        "concated = pd.concat([df1, df2])\n",
        "concated.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA87cxt7NlgY"
      },
      "source": [
        "concated_dt = concated[['label','text']]\n",
        "\n",
        "#sorting the rows by label\n",
        "concated_dt = concated_dt.sort_values(by=['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcHqYRPPNxXE"
      },
      "source": [
        "ax = concated_dt.groupby('label').count().plot(kind = 'bar', title='Distribution of data', legend=False)\n",
        "ax = ax.set_xticklabels(['Negative', 'Positive'], rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLTAntcCOIEW"
      },
      "source": [
        "concated_dt.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXT_0V0sOO-z"
      },
      "source": [
        "##### CLEANING THE DATA #####\n",
        "\n",
        "import re\n",
        "import string\n",
        "def cleaning(text):        \n",
        "    # converting to lowercase, removing URL links, special characters, punctuations...\n",
        "    text = text.lower()\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('[’“”…]', '', text)     \n",
        "    # removing the emojies              \n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)   \n",
        "    \n",
        "    # removing the stop-words          \n",
        "    text_tokens = word_tokenize(text)\n",
        "    stop_words = stopwords.words()\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
        "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "    text = filtered_sentence\n",
        "    \n",
        "    return text\n",
        "concated_dt['processed_text'] = concated_dt['text'].apply(cleaning)\n",
        "print(concated_dt['processed_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJffWtoTPCS6"
      },
      "source": [
        "#printing processed text with the corresponding original text\n",
        "count = 0\n",
        "for row in concated_dt.itertuples():\n",
        "  print(\"Text:\", row[2])\n",
        "  print(\"Processed:\", row[3])\n",
        "  count+=1\n",
        "  if count>10:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXkz1YYmaGY-"
      },
      "source": [
        "#splitting up the data into training and testing\n",
        "X_data, y_data = np.array(concated_dt['processed_text']), np.array(concated_dt['label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.05, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4R8lFI_aqrH"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "Embedding_dimenstions = 100\n",
        "\n",
        "Word2Vec_train_data = list(map(lambda x: x.split(), X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvVoK263bHLw"
      },
      "source": [
        "#word2vec model\n",
        "word_model = Word2Vec(Word2Vec_train_data, size=Embedding_dimenstions, workers=8, min_count=5)\n",
        "\n",
        "print(\"Vocabulary Length:\", len(word_model.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xk5iaFLbadc"
      },
      "source": [
        "input_length = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUhayVcSbnVo"
      },
      "source": [
        "#Tokenizing the sequence\n",
        "tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "\n",
        "vocab_length = len(tokenizer.word_index) + 1\n",
        "print(\"Tokenizer vocab length:\", vocab_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3F-ZSlObkH7"
      },
      "source": [
        "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n",
        "X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape :\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6nWxzjnbsfb"
      },
      "source": [
        "#Creating embedding matrix using Word2Vec model\n",
        "embedding_matrix = np.zeros((vocab_length, Embedding_dimenstions))\n",
        "\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    if word_model.wv.__contains__(word):\n",
        "        embedding_matrix[token] = word_model.wv.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5HpNGC8b7tm"
      },
      "source": [
        "#defining the model\n",
        "def getModel():\n",
        "    embedding_layer = Embedding(input_dim = vocab_length, \n",
        "                                output_dim = Embedding_dimenstions,\n",
        "                                weights=[embedding_matrix], \n",
        "                                input_length=input_length,\n",
        "                                trainable=False)\n",
        "\n",
        "    model = Sequential([\n",
        "        embedding_layer,\n",
        "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
        "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
        "        GlobalMaxPool1D(),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ],\n",
        "    name=\"Sentiment_Model\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBiG7FcicEZD"
      },
      "source": [
        "training_model = getModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5V3P6U-cINc"
      },
      "source": [
        "training_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuGm3hG3cPpr"
      },
      "source": [
        "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-7mkz0ycX0a"
      },
      "source": [
        "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSv_7LGclxK"
      },
      "source": [
        "#training the model\n",
        "history = training_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=500,\n",
        "    epochs=100,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UJ_7viNcwGZ"
      },
      "source": [
        "#training v/s validation accuracy graph\n",
        "acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\n",
        "loss, val_loss = history.history['loss'], history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, label='Trianing loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title(\"trainign and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yrlSf3kk7sh"
      },
      "source": [
        "def ConfusionMatrix(y_pred, y_test):\n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories  = ['Negative','Positive']\n",
        "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRQk-OIS9eqX"
      },
      "source": [
        "# Predicting on the Test dataset.\n",
        "y_pred = training_model.predict(X_test)\n",
        "\n",
        "# Converting prediction to reflect the sentiment predicted.\n",
        "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
        "\n",
        "# Printing out the Evaluation metrics. \n",
        "ConfusionMatrix(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GqZ02ez9nrS"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gei41RqQ9tPp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}